{"block_file": {"data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/ingest_green_taxi.py:data_loader:python:ingest green taxi": {"content": "#data loader para los taxis verdes para que vayan a el esquema bronze en snowflake\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport os, time, requests\nimport pyarrow.parquet as pq\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n\ndef need(name, val):\n    if not val:\n        raise Exception(f\"Falta el secret '{name}' en Mage (Settings \u2192 Secrets).\")\n    return val\n\n\ndef load_green_month_chunked(*, year:int, month:int, chunk_size:int=1_000_000, max_retries:int=3):\n    service    = \"green\"\n    dest_dir   = \"data/nyc_tlc\"\n    stage_name = \"TAXI_STAGE\"\n    table_name = \"GREEN_TRIPS\"             # tabla final de datos\n    meta_table = \"GREEN_TRIPS_METADATA\"    # tabla de metadatos\n    tmp_table  = \"_TMP_RAW_VARIANT\"        # staging temporal\n\n    os.makedirs(dest_dir, exist_ok=True)\n    fname = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\n    url = f\"{BASE_URL}/{fname}\"\n    local_path = os.path.join(dest_dir, fname)\n\n    # -------- credenciales --------\n    sf_user      = need(\"SNOWFLAKE_USER\",      get_secret_value(\"SNOWFLAKE_USER\"))\n    sf_password  = need(\"SNOWFLAKE_PASSWORD\",  get_secret_value(\"SNOWFLAKE_PASSWORD\"))\n    sf_account   = need(\"SNOWFLAKE_ACCOUNT\",   get_secret_value(\"SNOWFLAKE_ACCOUNT\"))\n    sf_warehouse = get_secret_value(\"SNOWFLAKE_WAREHOUSE\") or \"WH_INGEST\"\n    sf_database  = get_secret_value(\"SNOWFLAKE_DATABASE\")  or \"NYC_TAXI\"\n    sf_schema    = get_secret_value(\"SNOWFLAKE_SCHEMA\")    or \"RAW\"\n\n    conn = snowflake.connector.connect(\n        user=sf_user,\n        password=sf_password,\n        account=sf_account,\n        role=\"SYSADMIN\",\n        warehouse=sf_warehouse,\n        database=sf_database,\n        schema=sf_schema,\n        insecure_mode=True\n    )\n    cur = conn.cursor()\n\n    # -------- objetos base --------\n    cur.execute(f\"CREATE FILE FORMAT IF NOT EXISTS {sf_database}.{sf_schema}.PARQUET_FORMAT TYPE=PARQUET\")\n    cur.execute(f\"CREATE STAGE IF NOT EXISTS {sf_database}.{sf_schema}.{stage_name} \"\n                f\"FILE_FORMAT={sf_database}.{sf_schema}.PARQUET_FORMAT\")\n\n    # -------- descarga parquet --------\n    if not os.path.exists(local_path):\n        print(f\"Descargando {url} \u2026\")\n        r = requests.get(url, timeout=180)\n        if r.status_code == 404:\n            run_id = f\"{service}_{year}{month:02d}_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n            cur.execute(f\"\"\"\n                INSERT INTO {sf_database}.{sf_schema}.{meta_table}\n                (RUN_ID,SERVICE,YEAR,MONTH,SOURCE_FILE,STATUS,ERROR_MESSAGE,INGEST_TS)\n                VALUES (%s,%s,%s,%s,%s,%s,%s,CURRENT_TIMESTAMP())\n            \"\"\", (run_id, service, year, month, fname, 'MISSING', 'HTTP 404'))\n            cur.close(); conn.close()\n            print(f\"\u26a0\ufe0f Archivo no encontrado: {url}. Marcado MISSING en metadatos.\")\n            return {\"file\": fname, \"status\": \"MISSING\"}\n        r.raise_for_status()\n        with open(local_path, 'wb') as f:\n            f.write(r.content)\n\n    pf = pq.ParquetFile(local_path)\n    rows_in_file = pf.metadata.num_rows\n\n    # -------- subir al stage --------\n    print(f\"Subiendo {fname} al stage\u2026\")\n    cur.execute(f\"PUT file://{os.path.abspath(local_path)} \"\n                f\"@{sf_database}.{sf_schema}.{stage_name} OVERWRITE=TRUE\")\n\n    # -------- staging temporal --------\n    print(\"Creando tabla temporal staging \u2026\")\n    cur.execute(f\"CREATE OR REPLACE TEMP TABLE {tmp_table} (V VARIANT)\")\n\n    print(\"COPY INTO staging VARIANT \u2026\")\n    cur.execute(f\"\"\"\n        COPY INTO {tmp_table}(V)\n        FROM @{sf_database}.{sf_schema}.{stage_name}/{fname}\n        FILE_FORMAT = (TYPE=PARQUET)\n        ON_ERROR = ABORT_STATEMENT\n    \"\"\")\n\n    # -------- idempotencia --------\n    print(\"Eliminando datos previos de\", fname)\n    cur.execute(f\"DELETE FROM {sf_database}.{sf_schema}.{table_name} WHERE SOURCE_FILE = %s\", (fname,))\n\n    # -------- chunking con reintentos --------\n    total_inserted = 0\n    n_chunks = (rows_in_file + chunk_size - 1) // chunk_size\n    run_id_base = f\"{service}_{year}{month:02d}_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n\n    for chunk_index in range(1, n_chunks + 1):\n        start_rn = (chunk_index - 1) * chunk_size + 1\n        end_rn   = min(chunk_index * chunk_size, rows_in_file)\n        run_id   = f\"{run_id_base}_c{chunk_index}\"\n        print(f\"Chunk {chunk_index}/{n_chunks}: rn {start_rn}-{end_rn}\")\n\n        insert_sql = f\"\"\"\n        INSERT INTO {sf_database}.{sf_schema}.{table_name}\n        (VENDORID, LPEP_PICKUP_DATETIME, LPEP_DROPOFF_DATETIME, STORE_AND_FWD_FLAG,\n         RATECODEID, PULOCATIONID, DOLOCATIONID, PASSENGER_COUNT, TRIP_DISTANCE,\n         FARE_AMOUNT, EXTRA, MTA_TAX, TIP_AMOUNT, TOLLS_AMOUNT, EHAIL_FEE,\n         IMPROVEMENT_SURCHARGE, TOTAL_AMOUNT, PAYMENT_TYPE, TRIP_TYPE,\n         CONGESTION_SURCHARGE, CBD_CONGESTION_FEE, SOURCE_FILE)\n        SELECT\n            TRY_TO_NUMBER(v:VendorID::string),\n            TO_TIMESTAMP_NTZ(v:lpep_pickup_datetime::string),\n            TO_TIMESTAMP_NTZ(v:lpep_dropoff_datetime::string),\n            v:store_and_fwd_flag::string,\n            TRY_TO_NUMBER(v:RatecodeID::string),\n            TRY_TO_NUMBER(v:PULocationID::string),\n            TRY_TO_NUMBER(v:DOLocationID::string),\n            TRY_TO_NUMBER(v:passenger_count::string),\n            TRY_TO_DECIMAL(v:trip_distance::string, 12, 3),\n            TRY_TO_DECIMAL(v:fare_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:extra::string, 12, 2),\n            TRY_TO_DECIMAL(v:mta_tax::string, 12, 2),\n            TRY_TO_DECIMAL(v:tip_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:tolls_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:ehail_fee::string, 12, 2),\n            TRY_TO_DECIMAL(v:improvement_surcharge::string, 12, 2),\n            TRY_TO_DECIMAL(v:total_amount::string, 12, 2),\n            TRY_TO_NUMBER(v:payment_type::string),\n            TRY_TO_NUMBER(v:trip_type::string),\n            TRY_TO_DECIMAL(v:congestion_surcharge::string, 12, 2),\n            TRY_TO_DECIMAL(v:cbd_congestion_fee::string, 12, 2),\n            '{fname}'\n        FROM (\n            SELECT V, ROW_NUMBER() OVER (ORDER BY V:lpep_pickup_datetime::string) AS rn\n            FROM {tmp_table}\n        )\n        WHERE rn BETWEEN {start_rn} AND {end_rn}\n        \"\"\"\n\n        attempt = 0\n        success = False\n        while attempt < max_retries and not success:\n            try:\n                cur.execute(insert_sql)\n                inserted_chunk = cur.rowcount\n                total_inserted += inserted_chunk\n\n                cur.execute(f\"\"\"\n                    INSERT INTO {sf_database}.{sf_schema}.{meta_table}\n                    (RUN_ID,SERVICE,YEAR,MONTH,SOURCE_FILE,CHUNK_INDEX,CHUNK_SIZE,\n                     ROWS_IN_FILE,ROWS_INSERTED,STATUS,ERROR_MESSAGE,INGEST_TS)\n                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,CURRENT_TIMESTAMP())\n                \"\"\", (run_id, service, year, month, fname, chunk_index, chunk_size,\n                      rows_in_file, inserted_chunk, 'OK', None))\n                success = True\n            except Exception as e:\n                attempt += 1\n                if attempt >= max_retries:\n                    cur.execute(f\"\"\"\n                        INSERT INTO {sf_database}.{sf_schema}.{meta_table}\n                        (RUN_ID,SERVICE,YEAR,MONTH,SOURCE_FILE,CHUNK_INDEX,CHUNK_SIZE,\n                         ROWS_IN_FILE,ROWS_INSERTED,STATUS,ERROR_MESSAGE,INGEST_TS)\n                        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,CURRENT_TIMESTAMP())\n                    \"\"\", (run_id, service, year, month, fname, chunk_index, chunk_size,\n                          rows_in_file, 0, 'ERROR', str(e)))\n                    raise\n                else:\n                    print(f\"\u26a0\ufe0f Error en chunk {chunk_index} intento {attempt}: {e}, reintentando\u2026\")\n                    time.sleep(5)\n\n    cur.close()\n    conn.close()\n\n    print(f\"\u2705 {year}-{month:02d} cargado: {total_inserted}/{rows_in_file}\")\n    return {\n        \"file\": fname,\n        \"year\": year,\n        \"month\": month,\n        \"chunk_size\": chunk_size,\n        \"rows_in_file\": rows_in_file,\n        \"rows_inserted\": total_inserted\n    }\n\n\n@data_loader\ndef backfill_green_all_months(*args, **kwargs):\n    \"\"\"\n    Backfill completo de Green Taxi 2015-01 a 2025-12.\n    Idempotente y con reintentos en cada chunk.\n    \"\"\"\n    start_year, start_month = 2015, 1\n    end_year, end_month = 2025, 12\n\n    months = []\n    y, m = start_year, start_month\n    while (y < end_year) or (y == end_year and m <= end_month):\n        months.append((y, m))\n        if m == 12:\n            y += 1\n            m = 1\n        else:\n            m += 1\n\n    results = []\n    for (y, m) in months:\n        print(f\"\\n=== Procesando {y}-{m:02d} ===\")\n        try:\n            res = load_green_month_chunked(year=y, month=m, chunk_size=1_000_000, max_retries=3)\n            results.append({\"year\": y, \"month\": m, \"status\": \"OK\",\n                            \"rows_inserted\": res.get(\"rows_inserted\"),\n                            \"rows_in_file\": res.get(\"rows_in_file\")})\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error en {y}-{m:02d}: {e}\")\n            results.append({\"year\": y, \"month\": m, \"status\": \"ERROR\", \"error\": str(e)})\n\n    print(\"\\n\u2705 Backfill terminado\")\n    return results\n", "file_path": "data_loaders/ingest_green_taxi.py", "language": "python", "type": "data_loader", "uuid": "ingest_green_taxi"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/ny_yellow_taxi_ingest.py:data_loader:python:ny yellow taxi ingest": {"content": "#data loader para los taxis amarillos para que vayan a el esquema bronze en snowflake\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport os\nimport requests\nimport pyarrow.parquet as pq\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n\ndef need(name, val):\n    if not val:\n        raise Exception(f\"Falta el secret '{name}' en Mage (Settings \u2192 Secrets).\")\n    return val\n\ndef load_yellow_month_chunked_v2(*args, **kwargs):\n    \"\"\"\n    Carga UN mes de Yellow a BRONZE.YELLOW_TRIPS en chunks de size=1_000_000.\n    Paso 1: COPY INTO tabla staging TMP_RAW_VARIANT\n    Paso 2: INSERT INTO tabla final usando ROW_NUMBER() con rangos\n    \"\"\"\n    service    = \"yellow\"\n    year       = int(kwargs.get('year', 2015))\n    month      = int(kwargs.get('month', 1))\n    chunk_size = int(kwargs.get('chunk_size', 1_000_000))\n\n    dest_dir   = \"data/nyc_tlc\"\n    stage_name = \"TAXI_STAGE\"\n    table_name = \"YELLOW_TRIPS\"    # tabla final BRONZE\n    audit_tbl  = \"INGEST_AUDIT\"    # tabla de auditor\u00eda\n    tmp_table  = \"_TMP_RAW_VARIANT\"  # staging temporal para un mes\n\n    os.makedirs(dest_dir, exist_ok=True)\n    fname = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\n    url = f\"{BASE_URL}/{fname}\"\n    local_path = os.path.join(dest_dir, fname)\n\n    # -------- credenciales --------\n    sf_user      = need(\"SNOWFLAKE_USER\",      get_secret_value(\"SNOWFLAKE_USER\"))\n    sf_password  = need(\"SNOWFLAKE_PASSWORD\",  get_secret_value(\"SNOWFLAKE_PASSWORD\"))\n    sf_account   = need(\"SNOWFLAKE_ACCOUNT\",   get_secret_value(\"SNOWFLAKE_ACCOUNT\"))\n    sf_warehouse = get_secret_value(\"SNOWFLAKE_WAREHOUSE\") or \"WH_INGEST\"\n    sf_database  = get_secret_value(\"SNOWFLAKE_DATABASE\")  or \"NYC_TAXI\"\n    sf_schema    = get_secret_value(\"SNOWFLAKE_SCHEMA\")    or \"BRONZE\"\n\n    conn = snowflake.connector.connect(\n        user=sf_user,\n        password=sf_password,\n        account=sf_account,\n        role=\"SYSADMIN\",\n        warehouse=sf_warehouse,\n        database=sf_database,\n        schema=sf_schema,\n        insecure_mode=True\n    )\n    cur = conn.cursor()\n\n    # -------- objetos base --------\n    cur.execute(f\"CREATE FILE FORMAT IF NOT EXISTS {sf_database}.{sf_schema}.PARQUET_FORMAT TYPE=PARQUET\")\n    cur.execute(f\"CREATE STAGE IF NOT EXISTS {sf_database}.{sf_schema}.{stage_name} \"\n                f\"FILE_FORMAT={sf_database}.{sf_schema}.PARQUET_FORMAT\")\n\n    # -------- descarga parquet --------\n    rows_in_file = None\n    if not os.path.exists(local_path):\n        print(f\"Descargando {url} \u2026\")\n        r = requests.get(url, timeout=180)\n        if r.status_code == 404:\n            run_id = f\"{service}_{year}{month:02d}_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n            cur.execute(f\"\"\"\n                INSERT INTO {sf_database}.{sf_schema}.{audit_tbl}\n                (RUN_ID,SERVICE,YEAR,MONTH,SOURCE_FILE,CHUNK_INDEX,CHUNK_SIZE,ROWS_IN_FILE,ROWS_INSERTED,STATUS,ERROR_MESSAGE)\n                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n            \"\"\", (run_id, service, year, month, fname, None, chunk_size, None, 0, 'MISSING', 'HTTP 404'))\n            cur.close(); conn.close()\n            print(f\"\u26a0\ufe0f Archivo no encontrado: {url}. Marcado MISSING en auditor\u00eda.\")\n            return {\"file\": fname, \"status\": \"MISSING\"}\n        r.raise_for_status()\n        with open(local_path, 'wb') as f:\n            f.write(r.content)\n\n    pf = pq.ParquetFile(local_path)\n    rows_in_file = pf.metadata.num_rows\n\n    # -------- subir al stage --------\n    print(f\"Subiendo {fname} al stage\u2026\")\n    cur.execute(f\"PUT file://{os.path.abspath(local_path)} \"\n                f\"@{sf_database}.{sf_schema}.{stage_name} OVERWRITE=TRUE\")\n\n    # -------- staging temporal --------\n    print(\"Creando tabla temporal staging \u2026\")\n    cur.execute(f\"CREATE OR REPLACE TEMP TABLE {tmp_table} (V VARIANT)\")\n\n    print(\"COPY INTO staging VARIANT \u2026\")\n    cur.execute(f\"\"\"\n        COPY INTO {tmp_table}(V)\n        FROM @{sf_database}.{sf_schema}.{stage_name}/{fname}\n        FILE_FORMAT = (TYPE=PARQUET)\n        ON_ERROR = ABORT_STATEMENT\n    \"\"\")\n\n    # -------- idempotencia --------\n    cur.execute(f\"DELETE FROM {sf_database}.{sf_schema}.{table_name} WHERE SOURCE_FILE = %s\", (fname,))\n\n    # -------- chunking --------\n    total_inserted = 0\n    n_chunks = (rows_in_file + chunk_size - 1) // chunk_size\n    run_id_base = f\"{service}_{year}{month:02d}_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n\n    for chunk_index in range(1, n_chunks + 1):\n        start_rn = (chunk_index - 1) * chunk_size + 1\n        end_rn   = min(chunk_index * chunk_size, rows_in_file)\n        run_id   = f\"{run_id_base}_c{chunk_index}\"\n        print(f\"Chunk {chunk_index}/{n_chunks}: rn {start_rn}-{end_rn}\")\n\n        insert_sql = f\"\"\"\n        INSERT INTO {sf_database}.{sf_schema}.{table_name}\n        (VENDOR_ID, TPEP_PICKUP_DATETIME, TPEP_DROPOFF_DATETIME, PASSENGER_COUNT,\n         TRIP_DISTANCE, RATECODE_ID, STORE_AND_FWD_FLAG, PULOCATION_ID, DOLOCATION_ID,\n         PAYMENT_TYPE, FARE_AMOUNT, EXTRA, MTA_TAX, TIP_AMOUNT, TOLLS_AMOUNT,\n         IMPROVEMENT_SURCHARGE, TOTAL_AMOUNT, CONGESTION_SURCHARGE, AIRPORT_FEE,\n         CBD_CONGESTION_FEE, SOURCE_FILE)\n        SELECT\n            TRY_TO_NUMBER(v:VendorID::string),\n            TO_TIMESTAMP_NTZ(v:tpep_pickup_datetime::string),\n            TO_TIMESTAMP_NTZ(v:tpep_dropoff_datetime::string),\n            TRY_TO_NUMBER(v:passenger_count::string),\n            TRY_TO_DECIMAL(v:trip_distance::string, 12, 3),\n            TRY_TO_NUMBER(v:RatecodeID::string),\n            v:store_and_fwd_flag::string,\n            TRY_TO_NUMBER(v:PULocationID::string),\n            TRY_TO_NUMBER(v:DOLocationID::string),\n            TRY_TO_NUMBER(v:payment_type::string),\n            TRY_TO_DECIMAL(v:fare_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:extra::string, 12, 2),\n            TRY_TO_DECIMAL(v:mta_tax::string, 12, 2),\n            TRY_TO_DECIMAL(v:tip_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:tolls_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:improvement_surcharge::string, 12, 2),\n            TRY_TO_DECIMAL(v:total_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:congestion_surcharge::string, 12, 2),\n            TRY_TO_DECIMAL(COALESCE(v:Airport_fee::string, v:airport_fee::string), 12, 2),\n            TRY_TO_DECIMAL(v:cbd_congestion_fee::string, 12, 2),\n            '{fname}'\n        FROM (\n            SELECT V, ROW_NUMBER() OVER (ORDER BY V:tpep_pickup_datetime::string) AS rn\n            FROM {tmp_table}\n        )\n        WHERE rn BETWEEN {start_rn} AND {end_rn}\n        \"\"\"\n        try:\n            cur.execute(insert_sql)\n            inserted_chunk = cur.rowcount\n            total_inserted += inserted_chunk\n\n            cur.execute(f\"\"\"\n                INSERT INTO {sf_database}.{sf_schema}.{audit_tbl}\n                (RUN_ID,SERVICE,YEAR,MONTH,SOURCE_FILE,CHUNK_INDEX,CHUNK_SIZE,ROWS_IN_FILE,ROWS_INSERTED,STATUS,ERROR_MESSAGE)\n                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n            \"\"\", (run_id, service, year, month, fname, chunk_index, chunk_size, rows_in_file, inserted_chunk, 'OK', None))\n        except Exception as e:\n            cur.execute(f\"\"\"\n                INSERT INTO {sf_database}.{sf_schema}.{audit_tbl}\n                (RUN_ID,SERVICE,YEAR,MONTH,SOURCE_FILE,CHUNK_INDEX,CHUNK_SIZE,ROWS_IN_FILE,ROWS_INSERTED,STATUS,ERROR_MESSAGE)\n                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n            \"\"\", (run_id, service, year, month, fname, chunk_index, chunk_size, rows_in_file, 0, 'ERROR', str(e)))\n            raise\n\n    cur.close()\n    conn.close()\n\n    print(f\"\u2705 {year}-{month:02d} cargado: {total_inserted}/{rows_in_file}\")\n    return {\n        \"file\": fname,\n        \"year\": year,\n        \"month\": month,\n        \"chunk_size\": chunk_size,\n        \"rows_in_file\": rows_in_file,\n        \"rows_inserted\": total_inserted\n    }\n\n\n@data_loader\ndef backfill_yellow_all_months(*args, **kwargs):\n    \"\"\"\n    Llama a load_yellow_month_chunked_v2 por cada mes desde 2015-01 hasta 2025-08.\n    Idempotente: si ya se carg\u00f3 un mes, vuelve a borrar y reinsertar.\n    \"\"\"\n\n    # --- IMPORTAMOS LA FUNCI\u00d3N PRINCIPAL DEL MISMO ARCHIVO ---\n    \n    start_year, start_month = 2015, 1\n    end_year, end_month = 2025, 8\n\n    # Construir lista de meses\n    months = []\n    y, m = start_year, start_month\n    while (y < end_year) or (y == end_year and m <= end_month):\n        months.append((y, m))\n        if m == 12:\n            y += 1\n            m = 1\n        else:\n            m += 1\n\n    results = []\n    for (y, m) in months:\n        print(f\"\\n=== Procesando {y}-{m:02d} ===\")\n        try:\n            res = load_yellow_month_chunked_v2(year=y, month=m, chunk_size=1_000_000)\n            results.append({\n                \"year\": y,\n                \"month\": m,\n                \"status\": \"OK\",\n                \"rows_inserted\": res.get(\"rows_inserted\"),\n                \"rows_in_file\": res.get(\"rows_in_file\")\n            })\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error en {y}-{m:02d}: {e}\")\n            results.append({\n                \"year\": y,\n                \"month\": m,\n                \"status\": \"ERROR\",\n                \"error\": str(e)\n            })\n\n    print(\"\\n\u2705 Backfill terminado\")\n    return results\n", "file_path": "data_loaders/ny_yellow_taxi_ingest.py", "language": "python", "type": "data_loader", "uuid": "ny_yellow_taxi_ingest"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_age = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_age)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/ny_yellow_taxi/metadata.yaml:pipeline:yaml:ny yellow taxi/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ny_yellow_taxi_ingest\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ny_yellow_taxi_ingest\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_green_taxi\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_green_taxi\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-29 20:33:39.313984+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: ny_yellow_taxi\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: ny_yellow_taxi\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/ny_yellow_taxi/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "ny_yellow_taxi/metadata"}, "pipelines/ny_yellow_taxi/__init__.py:pipeline:python:ny yellow taxi/  init  ": {"content": "", "file_path": "pipelines/ny_yellow_taxi/__init__.py", "language": "python", "type": "pipeline", "uuid": "ny_yellow_taxi/__init__"}, "/home/src/scheduler/data_loaders/taxi_zones_ingest.py:data_loader:python:home/src/scheduler/data loaders/taxi zones ingest": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport os\nimport requests\nimport csv\nimport snowflake.connector\nimport time\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nTAXI_ZONES_URL = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\nLOCAL_CSV = \"data/taxi_zones.csv\"\n\n\ndef need(name, val):\n    if not val:\n        raise Exception(f\"Falta el secret '{name}' en Mage (Settings \u2192 Secrets).\")\n    return val\n\n\n# === Funciones de ayuda con reintentos ===\ndef retry_request(url, retries=3, delay=5):\n    \"\"\"\n    Descarga un archivo con reintentos exponenciales.\n    \"\"\"\n    for i in range(1, retries + 1):\n        try:\n            r = requests.get(url, timeout=120)\n            r.raise_for_status()\n            return r\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error en intento {i}/{retries} al descargar {url}: {e}\")\n            if i < retries:\n                time.sleep(delay * i)  # backoff exponencial\n            else:\n                raise\n\n\ndef retry_snowflake_op(cur, sql, params=None, retries=3, delay=5):\n    \"\"\"\n    Ejecuta una query Snowflake con reintentos si hay error temporal.\n    \"\"\"\n    for i in range(1, retries + 1):\n        try:\n            if params:\n                cur.execute(sql, params)\n            else:\n                cur.execute(sql)\n            return\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error Snowflake intento {i}/{retries}: {e}\")\n            if i < retries:\n                time.sleep(delay * i)\n            else:\n                raise\n\n\n@data_loader\ndef load_taxi_zones(*args, **kwargs):\n    \"\"\"\n    Descarga el CSV oficial de taxi zones y lo inserta en BRONZE.TAXI_ZONES.\n    Idempotente + reintentos + inserci\u00f3n en lotes.\n    \"\"\"\n    # ---------- credenciales ----------\n    sf_user      = need(\"SNOWFLAKE_USER\",      get_secret_value(\"SNOWFLAKE_USER\"))\n    sf_password  = need(\"SNOWFLAKE_PASSWORD\",  get_secret_value(\"SNOWFLAKE_PASSWORD\"))\n    sf_account   = need(\"SNOWFLAKE_ACCOUNT\",   get_secret_value(\"SNOWFLAKE_ACCOUNT\"))\n    sf_warehouse = get_secret_value(\"SNOWFLAKE_WAREHOUSE\") or \"WH_INGEST\"\n    sf_database  = get_secret_value(\"SNOWFLAKE_DATABASE\")  or \"NYC_TAXI\"\n    sf_schema    = 'SILVER'\n\n    # ---------- conexi\u00f3n Snowflake ----------\n    conn = snowflake.connector.connect(\n        user=sf_user,\n        password=sf_password,\n        account=sf_account,\n        role=\"SYSADMIN\",\n        warehouse=sf_warehouse,\n        database=sf_database,\n        schema=sf_schema,\n        insecure_mode=True  # qu\u00edtalo si ya resolviste certificados\n    )\n    cur = conn.cursor()\n\n    # ---------- descarga del CSV con reintentos ----------\n    os.makedirs(\"data\", exist_ok=True)\n    if not os.path.exists(LOCAL_CSV):\n        print(f\"Descargando {TAXI_ZONES_URL} \u2026\")\n        r = retry_request(TAXI_ZONES_URL, retries=5, delay=5)\n        with open(LOCAL_CSV, \"wb\") as f:\n            f.write(r.content)\n\n    # ---------- crear tabla si no existe ----------\n    retry_snowflake_op(cur, f\"\"\"\n        CREATE TABLE IF NOT EXISTS {sf_database}.{sf_schema}.TAXI_ZONES (\n            LOCATIONID    NUMBER(38,0),\n            BOROUGH       STRING,\n            ZONE          STRING,\n            SERVICE_ZONE  STRING\n        )\n    \"\"\")\n\n    # ---------- limpiar tabla para idempotencia ----------\n    retry_snowflake_op(cur, f\"TRUNCATE TABLE {sf_database}.{sf_schema}.TAXI_ZONES\")\n\n    # ---------- insertar datos en lotes ----------\n    batch_size = 1000  # puedes cambiarlo si quieres\n    batch = []\n    inserted = 0\n\n    with open(LOCAL_CSV, newline='', encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            batch.append((\n                int(row[\"LocationID\"]) if row.get(\"LocationID\") else None,\n                row.get(\"Borough\"),\n                row.get(\"Zone\"),\n                row.get(\"service_zone\"),\n            ))\n\n            if len(batch) >= batch_size:\n                retry_snowflake_op(\n                    cur,\n                    f\"INSERT INTO {sf_database}.{sf_schema}.TAXI_ZONES (LOCATIONID,BOROUGH,ZONE,SERVICE_ZONE) VALUES \" +\n                    \", \".join([\"(%s,%s,%s,%s)\"] * len(batch)),\n                    [val for tup in batch for val in tup]\n                )\n                inserted += len(batch)\n                batch = []\n\n    # Inserta el \u00faltimo batch si qued\u00f3 algo\n    if batch:\n        retry_snowflake_op(\n            cur,\n            f\"INSERT INTO {sf_database}.{sf_schema}.TAXI_ZONES (LOCATIONID,BOROUGH,ZONE,SERVICE_ZONE) VALUES \" +\n            \", \".join([\"(%s,%s,%s,%s)\"] * len(batch)),\n            [val for tup in batch for val in tup]\n        )\n        inserted += len(batch)\n\n    cur.close()\n    conn.close()\n\n    print(f\"\u2705 Cargadas {inserted} filas en {sf_database}.{sf_schema}.TAXI_ZONES (con reintentos y batch insert)\")\n    return {\"rows_inserted\": inserted}\n", "file_path": "/home/src/scheduler/data_loaders/taxi_zones_ingest.py", "language": "python", "type": "data_loader", "uuid": "taxi_zones_ingest"}, "/home/src/scheduler/dbt/models/staging/stg_zones.sql:dbt:sql:home/src/scheduler/dbt/models/staging/stg zones": {"content": "select \n* \nfrom {{source('bronze','TAXI_ZONES')}}", "file_path": "/home/src/scheduler/dbt/models/staging/stg_zones.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/staging/stg_zones"}, "/home/src/scheduler/data_loaders/yellow_taxis_silver.py:data_loader:python:home/src/scheduler/data loaders/yellow taxis silver": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport os, time, requests\nimport pyarrow.parquet as pq\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n\ndef need(name, val):\n    if not val:\n        raise Exception(f\"Falta el secret '{name}' en Mage (Settings \u2192 Secrets).\")\n    return val\n\n\ndef load_green_to_silver(*, year:int, month:int, chunk_size:int=1_000_000, max_retries:int=3):\n    \"\"\"\n    Carga un mes de Green Taxi a SILVER.TAXI_TRIPS_ALL con limpieza y estandarizaci\u00f3n.\n    \"\"\"\n    service    = \"green\"\n    dest_dir   = \"data/nyc_tlc\"\n    stage_name = \"TAXI_STAGE\"\n    tmp_table  = \"_TMP_RAW_VARIANT\"        # staging temporal\n\n    os.makedirs(dest_dir, exist_ok=True)\n    fname = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\n    url = f\"{BASE_URL}/{fname}\"\n    local_path = os.path.join(dest_dir, fname)\n\n    # -------- credenciales --------\n    sf_user      = need(\"SNOWFLAKE_USER\",      get_secret_value(\"SNOWFLAKE_USER\"))\n    sf_password  = need(\"SNOWFLAKE_PASSWORD\",  get_secret_value(\"SNOWFLAKE_PASSWORD\"))\n    sf_account   = need(\"SNOWFLAKE_ACCOUNT\",   get_secret_value(\"SNOWFLAKE_ACCOUNT\"))\n    sf_warehouse = get_secret_value(\"SNOWFLAKE_WAREHOUSE\") or \"WH_INGEST\"\n    sf_database  = get_secret_value(\"SNOWFLAKE_DATABASE\")  or \"NYC_TAXI\"\n\n    conn = snowflake.connector.connect(\n        user=sf_user,\n        password=sf_password,\n        account=sf_account,\n        role=\"SYSADMIN\",  # o INGEST_ROLE si ya tiene permisos\n        warehouse=sf_warehouse,\n        database=sf_database,\n        schema=\"SILVER\",\n        insecure_mode=True\n    )\n    cur = conn.cursor()\n\n    # -------- objetos base --------\n    cur.execute(f\"CREATE FILE FORMAT IF NOT EXISTS {sf_database}.SILVER.PARQUET_FORMAT TYPE=PARQUET\")\n    cur.execute(f\"CREATE STAGE IF NOT EXISTS {sf_database}.SILVER.{stage_name} \"\n                f\"FILE_FORMAT={sf_database}.SILVER.PARQUET_FORMAT\")\n\n    # -------- descarga parquet --------\n    if not os.path.exists(local_path):\n        print(f\"Descargando {url} \u2026\")\n        r = requests.get(url, timeout=180)\n        if r.status_code == 404:\n            print(f\"\u26a0\ufe0f Archivo no encontrado: {url}\")\n            return {\"file\": fname, \"status\": \"MISSING\"}\n        r.raise_for_status()\n        with open(local_path, 'wb') as f:\n            f.write(r.content)\n\n    pf = pq.ParquetFile(local_path)\n    rows_in_file = pf.metadata.num_rows\n    print(f\"Archivo {fname} con {rows_in_file} filas\")\n\n    # -------- subir al stage --------\n    print(f\"Subiendo {fname} al stage\u2026\")\n    cur.execute(f\"PUT file://{os.path.abspath(local_path)} \"\n                f\"@{sf_database}.SILVER.{stage_name} OVERWRITE=TRUE\")\n\n    # -------- staging temporal --------\n    print(\"Creando tabla temporal staging \u2026\")\n    cur.execute(f\"CREATE OR REPLACE TEMP TABLE {tmp_table} (V VARIANT)\")\n\n    print(\"COPY INTO staging VARIANT \u2026\")\n    cur.execute(f\"\"\"\n        COPY INTO {tmp_table}(V)\n        FROM @{sf_database}.SILVER.{stage_name}/{fname}\n        FILE_FORMAT = (TYPE=PARQUET)\n        ON_ERROR = ABORT_STATEMENT\n    \"\"\")\n\n    # -------- idempotencia --------\n    print(\"Eliminando datos previos de\", fname)\n    cur.execute(f\"DELETE FROM {sf_database}.SILVER.TAXI_TRIPS_ALL WHERE SOURCE_FILE = %s\", (fname,))\n\n    # -------- chunking con reintentos --------\n    total_inserted = 0\n    n_chunks = (rows_in_file + chunk_size - 1) // chunk_size\n\n    for chunk_index in range(1, n_chunks + 1):\n        start_rn = (chunk_index - 1) * chunk_size + 1\n        end_rn   = min(chunk_index * chunk_size, rows_in_file)\n        print(f\"Chunk {chunk_index}/{n_chunks}: rn {start_rn}-{end_rn}\")\n\n        insert_sql = f\"\"\"\n        INSERT INTO {sf_database}.SILVER.TAXI_TRIPS_ALL\n        (VENDOR_ID, PICKUP_DATETIME, DROPOFF_DATETIME, PASSENGER_COUNT,\n         TRIP_DISTANCE, RATECODE_ID, STORE_AND_FWD_FLAG, PULOCATION_ID, DOLOCATION_ID,\n         PAYMENT_TYPE_ID, PAYMENT_TYPE, FARE_AMOUNT, EXTRA, MTA_TAX, TIP_AMOUNT, TOLLS_AMOUNT,\n         IMPROVEMENT_SURCHARGE, TOTAL_AMOUNT, CONGESTION_SURCHARGE, AIRPORT_FEE, CBD_CONGESTION_FEE,\n         EHAIL_FEE, TRIP_TYPE, SERVICE_TYPE, SOURCE_FILE, LOAD_TS)\n        SELECT\n            TRY_TO_NUMBER(v:VendorID::string),\n            TO_TIMESTAMP_NTZ(v:lpep_pickup_datetime::string),\n            TO_TIMESTAMP_NTZ(v:lpep_dropoff_datetime::string),\n            NULLIF(TRY_TO_NUMBER(v:passenger_count::string),0),\n            TRY_TO_DECIMAL(v:trip_distance::string, 12, 3),\n            TRY_TO_NUMBER(v:RatecodeID::string),\n            v:store_and_fwd_flag::string,\n            TRY_TO_NUMBER(v:PULocationID::string),\n            TRY_TO_NUMBER(v:DOLocationID::string),\n            TRY_TO_NUMBER(v:payment_type::string),\n            CASE TRY_TO_NUMBER(v:payment_type::string)\n                WHEN 1 THEN 'Credit Card'\n                WHEN 2 THEN 'Cash'\n                WHEN 3 THEN 'No Charge'\n                WHEN 4 THEN 'Dispute'\n                WHEN 5 THEN 'Unknown'\n                WHEN 6 THEN 'Voided Trip'\n                ELSE 'Other'\n            END,\n            TRY_TO_DECIMAL(v:fare_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:extra::string, 12, 2),\n            TRY_TO_DECIMAL(v:mta_tax::string, 12, 2),\n            TRY_TO_DECIMAL(v:tip_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:tolls_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:improvement_surcharge::string, 12, 2),\n            TRY_TO_DECIMAL(v:total_amount::string, 12, 2),\n            TRY_TO_DECIMAL(v:congestion_surcharge::string, 12, 2),\n            NULL,\n            TRY_TO_DECIMAL(v:cbd_congestion_fee::string, 12, 2),\n            TRY_TO_DECIMAL(v:ehail_fee::string, 12, 2),\n            TRY_TO_NUMBER(v:trip_type::string),\n            'green',\n            '{fname}',\n            CURRENT_TIMESTAMP()\n        FROM (\n            SELECT V, ROW_NUMBER() OVER (ORDER BY V:lpep_pickup_datetime::string) AS rn\n            FROM {tmp_table}\n        )\n        WHERE rn BETWEEN {start_rn} AND {end_rn}\n          AND v:lpep_pickup_datetime IS NOT NULL\n          AND v:lpep_dropoff_datetime IS NOT NULL\n          AND TRY_TO_DECIMAL(v:trip_distance::string,12,3) >= 0\n          AND TRY_TO_DECIMAL(v:total_amount::string,12,2) >= 0\n        \"\"\"\n\n        attempt = 0\n        success = False\n        while attempt < max_retries and not success:\n            try:\n                cur.execute(insert_sql)\n                inserted_chunk = cur.rowcount\n                total_inserted += inserted_chunk\n                success = True\n            except Exception as e:\n                attempt += 1\n                if attempt >= max_retries:\n                    print(f\"\u274c Error definitivo en chunk {chunk_index}: {e}\")\n                    raise\n                else:\n                    print(f\"\u26a0\ufe0f Error en chunk {chunk_index} intento {attempt}: {e}, reintentando\u2026\")\n                    time.sleep(5)\n\n    cur.close()\n    conn.close()\n\n    print(f\"\u2705 {year}-{month:02d} cargado a SILVER.TAXI_TRIPS_ALL: {total_inserted}/{rows_in_file}\")\n    return {\n        \"file\": fname,\n        \"year\": year,\n        \"month\": month,\n        \"chunk_size\": chunk_size,\n        \"rows_in_file\": rows_in_file,\n        \"rows_inserted\": total_inserted\n    }\n\n\n@data_loader\ndef backfill_green_silver_all_months(*args, **kwargs):\n    \"\"\"\n    Backfill completo de Green Taxi a SILVER.TAXI_TRIPS_ALL (2015-01 \u2192 2025-12)\n    \"\"\"\n    start_year, start_month = 2015, 1\n    end_year, end_month = 2025, 12\n\n    months = []\n    y, m = start_year, start_month\n    while (y < end_year) or (y == end_year and m <= end_month):\n        months.append((y, m))\n        if m == 12:\n            y += 1\n            m = 1\n        else:\n            m += 1\n\n    results = []\n    for (y, m) in months:\n        print(f\"\\n=== Procesando {y}-{m:02d} ===\")\n        try:\n            res = load_green_to_silver(year=y, month=m, chunk_size=1_000_000, max_retries=3)\n            results.append({\"year\": y, \"month\": m, \"status\": \"OK\",\n                            \"rows_inserted\": res.get(\"rows_inserted\"),\n                            \"rows_in_file\": res.get(\"rows_in_file\")})\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error en {y}-{m:02d}: {e}\")\n            results.append({\"year\": y, \"month\": m, \"status\": \"ERROR\", \"error\": str(e)})\n\n    print(\"\\n\u2705 Backfill terminado\")\n    return results\n", "file_path": "/home/src/scheduler/data_loaders/yellow_taxis_silver.py", "language": "python", "type": "data_loader", "uuid": "yellow_taxis_silver"}, "/home/src/scheduler/data_loaders/silver_all_yellow_trips.py:data_loader:python:home/src/scheduler/data loaders/silver all yellow trips": {"content": "if 'data_loader' not in globals():\r\n    from mage_ai.data_preparation.decorators import data_loader\r\n\r\nimport os, time, requests\r\nimport pyarrow.parquet as pq\r\nimport snowflake.connector\r\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\r\n\r\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\r\n\r\ndef need(name, val):\r\n    if not val:\r\n        raise Exception(f\"Falta el secret '{name}' en Mage (Settings \u2192 Secrets).\")\r\n    return val\r\n\r\ndef load_yellow_to_silver(*, year:int, month:int, chunk_size:int=1_000_000, max_retries:int=3):\r\n    \"\"\"\r\n    Carga un mes de Yellow Taxi a SILVER.TAXI_TRIPS_ALL con limpieza y estandarizaci\u00f3n.\r\n    \"\"\"\r\n    service    = \"yellow\"\r\n    dest_dir   = \"data/nyc_tlc\"\r\n    stage_name = \"TAXI_STAGE\"\r\n    tmp_table  = \"_TMP_RAW_VARIANT\"\r\n\r\n    os.makedirs(dest_dir, exist_ok=True)\r\n    fname = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\r\n    url = f\"{BASE_URL}/{fname}\"\r\n    local_path = os.path.join(dest_dir, fname)\r\n\r\n    # --- credenciales ---\r\n    sf_user      = need(\"SNOWFLAKE_USER\",      get_secret_value(\"SNOWFLAKE_USER\"))\r\n    sf_password  = need(\"SNOWFLAKE_PASSWORD\",  get_secret_value(\"SNOWFLAKE_PASSWORD\"))\r\n    sf_account   = need(\"SNOWFLAKE_ACCOUNT\",   get_secret_value(\"SNOWFLAKE_ACCOUNT\"))\r\n    sf_warehouse = get_secret_value(\"SNOWFLAKE_WAREHOUSE\") or \"WH_INGEST\"\r\n    sf_database  = get_secret_value(\"SNOWFLAKE_DATABASE\")  or \"NYC_TAXI\"\r\n\r\n    conn = snowflake.connector.connect(\r\n        user=sf_user,\r\n        password=sf_password,\r\n        account=sf_account,\r\n        role=\"SYSADMIN\",\r\n        warehouse=sf_warehouse,\r\n        database=sf_database,\r\n        schema=\"SILVER\",\r\n        insecure_mode = True\r\n    )\r\n    cur = conn.cursor()\r\n\r\n    # --- objetos base ---\r\n    cur.execute(f\"CREATE FILE FORMAT IF NOT EXISTS {sf_database}.SILVER.PARQUET_FORMAT TYPE=PARQUET\")\r\n    cur.execute(f\"CREATE STAGE IF NOT EXISTS {sf_database}.SILVER.{stage_name} \"\r\n                f\"FILE_FORMAT={sf_database}.SILVER.PARQUET_FORMAT\")\r\n\r\n    # --- descarga parquet ---\r\n    if not os.path.exists(local_path):\r\n        print(f\"Descargando {url} \u2026\")\r\n        r = requests.get(url, timeout=180)\r\n        if r.status_code == 404:\r\n            print(f\"\u26a0\ufe0f Archivo no encontrado: {url}\")\r\n            return {\"file\": fname, \"status\": \"MISSING\"}\r\n        r.raise_for_status()\r\n        with open(local_path, 'wb') as f:\r\n            f.write(r.content)\r\n\r\n    pf = pq.ParquetFile(local_path)\r\n    rows_in_file = pf.metadata.num_rows\r\n    print(f\"Archivo {fname} con {rows_in_file} filas\")\r\n\r\n    # --- subir al stage ---\r\n    cur.execute(f\"PUT file://{os.path.abspath(local_path)} \"\r\n                f\"@{sf_database}.SILVER.{stage_name} OVERWRITE=TRUE\")\r\n\r\n    # --- staging temporal ---\r\n    cur.execute(f\"CREATE OR REPLACE TEMP TABLE {tmp_table} (V VARIANT)\")\r\n    cur.execute(f\"\"\"\r\n        COPY INTO {tmp_table}(V)\r\n        FROM @{sf_database}.SILVER.{stage_name}/{fname}\r\n        FILE_FORMAT = (TYPE=PARQUET)\r\n        ON_ERROR = ABORT_STATEMENT\r\n    \"\"\")\r\n\r\n    # --- idempotencia ---\r\n    cur.execute(f\"DELETE FROM {sf_database}.SILVER.TAXI_TRIPS_ALL WHERE SOURCE_FILE = %s\", (fname,))\r\n\r\n    # --- chunking ---\r\n    total_inserted = 0\r\n    n_chunks = (rows_in_file + chunk_size - 1) // chunk_size\r\n\r\n    for chunk_index in range(1, n_chunks + 1):\r\n        start_rn = (chunk_index - 1) * chunk_size + 1\r\n        end_rn   = min(chunk_index * chunk_size, rows_in_file)\r\n        print(f\"Chunk {chunk_index}/{n_chunks}: filas {start_rn}-{end_rn}\")\r\n\r\n        insert_sql = f\"\"\"\r\n            INSERT INTO {sf_database}.SILVER.TAXI_TRIPS_ALL\r\n            (AIRPORT_FEE, CBD_CONGESTION_FEE, CONGESTION_SURCHARGE,\r\n            DOLOCATION_ID, DROPOFF_DATETIME, EHAIL_FEE, EXTRA, FARE_AMOUNT,\r\n            IMPROVEMENT_SURCHARGE, LOAD_TS, MTA_TAX, PASSENGER_COUNT,\r\n            PAYMENT_TYPE, PAYMENT_TYPE_ID, PICKUP_DATETIME, PULOCATION_ID,\r\n            RATECODE_ID, SERVICE_TYPE, SOURCE_FILE, STORE_AND_FWD_FLAG,\r\n            TIP_AMOUNT, TOLLS_AMOUNT, TOTAL_AMOUNT, TRIP_DISTANCE, TRIP_TYPE,\r\n            VENDOR_ID)\r\n            SELECT\r\n                TRY_TO_DECIMAL(v:Airport_fee::string, 12, 2)                         AS AIRPORT_FEE,\r\n                TRY_TO_DECIMAL(v:cbd_congestion_fee::string, 12, 2)                  AS CBD_CONGESTION_FEE,\r\n                TRY_TO_DECIMAL(v:congestion_surcharge::string, 12, 2)                AS CONGESTION_SURCHARGE,\r\n                TRY_TO_NUMBER(v:DOLocationID::string)                                AS DOLOCATION_ID,\r\n                TO_TIMESTAMP_NTZ(v:tpep_dropoff_datetime::string)                    AS DROPOFF_DATETIME,\r\n                NULL                                                                 AS EHAIL_FEE,\r\n                TRY_TO_DECIMAL(v:extra::string, 12, 2)                               AS EXTRA,\r\n                TRY_TO_DECIMAL(v:fare_amount::string, 12, 2)                         AS FARE_AMOUNT,\r\n                TRY_TO_DECIMAL(v:improvement_surcharge::string, 12, 2)               AS IMPROVEMENT_SURCHARGE,\r\n                CURRENT_TIMESTAMP()                                                  AS LOAD_TS,\r\n                TRY_TO_DECIMAL(v:mta_tax::string, 12, 2)                             AS MTA_TAX,\r\n                NULLIF(TRY_TO_NUMBER(v:passenger_count::string),0)                   AS PASSENGER_COUNT,\r\n                CASE TRY_TO_NUMBER(v:payment_type::string)\r\n                    WHEN 1 THEN 'Credit Card'\r\n                    WHEN 2 THEN 'Cash'\r\n                    WHEN 3 THEN 'No Charge'\r\n                    WHEN 4 THEN 'Dispute'\r\n                    WHEN 5 THEN 'Unknown'\r\n                    WHEN 6 THEN 'Voided Trip'\r\n                    ELSE 'Other'\r\n                END                                                                  AS PAYMENT_TYPE,\r\n                TRY_TO_NUMBER(v:payment_type::string)                                AS PAYMENT_TYPE_ID,\r\n                TO_TIMESTAMP_NTZ(v:tpep_pickup_datetime::string)                     AS PICKUP_DATETIME,\r\n                TRY_TO_NUMBER(v:PULocationID::string)                                AS PULOCATION_ID,\r\n                TRY_TO_NUMBER(v:RatecodeID::string)                                  AS RATECODE_ID,\r\n                'yellow'                                                             AS SERVICE_TYPE,\r\n                '{fname}'                                                            AS SOURCE_FILE,\r\n                v:store_and_fwd_flag::string                                         AS STORE_AND_FWD_FLAG,\r\n                TRY_TO_DECIMAL(v:tip_amount::string, 12, 2)                          AS TIP_AMOUNT,\r\n                TRY_TO_DECIMAL(v:tolls_amount::string, 12, 2)                        AS TOLLS_AMOUNT,\r\n                TRY_TO_DECIMAL(v:total_amount::string, 12, 2)                        AS TOTAL_AMOUNT,\r\n                TRY_TO_DECIMAL(v:trip_distance::string, 12, 3)                       AS TRIP_DISTANCE,\r\n                TRY_TO_NUMBER(v:trip_type::string)                                   AS TRIP_TYPE,\r\n                TRY_TO_NUMBER(v:VendorID::string)                                    AS VENDOR_ID\r\n            FROM (\r\n                SELECT V, ROW_NUMBER() OVER (ORDER BY V:tpep_pickup_datetime::string) AS rn\r\n                FROM {tmp_table}\r\n            )\r\n            WHERE rn BETWEEN {start_rn} AND {end_rn}\r\n            -- Reglas de calidad m\u00ednimas:\r\n            AND v:tpep_pickup_datetime IS NOT NULL\r\n            AND v:tpep_dropoff_datetime IS NOT NULL\r\n            AND TRY_TO_DECIMAL(v:trip_distance::string,12,3) >= 0\r\n            AND TRY_TO_DECIMAL(v:total_amount::string,12,2) >= 0\r\n            AND DATEDIFF('hour', TO_TIMESTAMP_NTZ(v:tpep_pickup_datetime::string),\r\n                                TO_TIMESTAMP_NTZ(v:tpep_dropoff_datetime::string)) <= 24\r\n            \"\"\"\r\n\r\n\r\n        attempt = 0\r\n        while attempt < max_retries:\r\n            try:\r\n                cur.execute(insert_sql)\r\n                total_inserted += cur.rowcount\r\n                break\r\n            except Exception as e:\r\n                attempt += 1\r\n                if attempt == max_retries:\r\n                    print(f\"\u274c Error definitivo en chunk {chunk_index}: {e}\")\r\n                    raise\r\n                else:\r\n                    print(f\"\u26a0\ufe0f Error en chunk {chunk_index} intento {attempt}: {e}, reintentando\u2026\")\r\n                    time.sleep(5)\r\n\r\n    cur.close()\r\n    conn.close()\r\n    print(f\"\u2705 {year}-{month:02d}: {total_inserted}/{rows_in_file} filas insertadas\")\r\n    return {\"file\": fname, \"rows_inserted\": total_inserted, \"rows_in_file\": rows_in_file}\r\n\r\n@data_loader\r\ndef backfill_yellow_silver_all_months(*args, **kwargs):\r\n    \"\"\"\r\n    Backfill completo Yellow Taxi \u2192 SILVER.TAXI_TRIPS_ALL (2015-01 \u2192 2025-12)\r\n    \"\"\"\r\n    start_year, start_month = 2015, 1\r\n    end_year, end_month = 2025, 12\r\n    y, m = start_year, start_month\r\n    results = []\r\n\r\n    while (y < end_year) or (y == end_year and m <= end_month):\r\n        print(f\"\\n=== Procesando {y}-{m:02d} ===\")\r\n        try:\r\n            res = load_yellow_to_silver(year=y, month=m)\r\n            results.append({\"year\": y, \"month\": m, \"status\": \"OK\", **res})\r\n        except Exception as e:\r\n            results.append({\"year\": y, \"month\": m, \"status\": \"ERROR\", \"error\": str(e)})\r\n        if m == 12:\r\n            y += 1; m = 1\r\n        else:\r\n            m += 1\r\n    print(\"\\n\u2705 Backfill terminado\")\r\n    return results\r\n", "file_path": "/home/src/scheduler/data_loaders/silver_all_yellow_trips.py", "language": "python", "type": "data_loader", "uuid": "silver_all_yellow_trips"}, "/home/src/scheduler/dbt/models/staging/stg_yellow_trips.sql:dbt:sql:home/src/scheduler/dbt/models/staging/stg yellow trips": {"content": "{{ config(\n    materialized='table'\n) }}\n\nselect\n    vendorid as vendor_id,\n    tpep_pickup_datetime as pickup_datetime,\n    tpep_dropoff_datetime as dropoff_datetime,\n    passenger_count,\n    trip_distance,\n    ratecodeid as rate_code_id,\n    store_and_fwd_flag,\n    pulocationid as pickup_location_id,\n    dolocationid as dropoff_location_id,\n    payment_type,\n    fare_amount,\n    extra,\n    mta_tax,\n    tip_amount,\n    tolls_amount,\n    improvement_surcharge,\n    total_amount,\n    congestion_surcharge,\n    airport_fee\nfrom {{ source('bronze','yellow_trips') }}\n", "file_path": "/home/src/scheduler/dbt/models/staging/stg_yellow_trips.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/staging/stg_yellow_trips"}, "/home/src/scheduler/dbt/models/staging/stg_green_trips.sql:dbt:sql:home/src/scheduler/dbt/models/staging/stg green trips": {"content": "{{ config(\n    materialized='table'\n) }}\n\nselect\n    vendorid as vendor_id,\n    lpep_pickup_datetime as pickup_datetime,\n    lpep_dropoff_datetime as dropoff_datetime,\n    passenger_count,\n    trip_distance,\n    ratecodeid as rate_code_id,\n    store_and_fwd_flag,\n    pulocationid as pickup_location_id,\n    dolocationid as dropoff_location_id,\n    payment_type,\n    fare_amount,\n    extra,\n    mta_tax,\n    tip_amount,\n    tolls_amount,\n    improvement_surcharge,\n    total_amount,\n    congestion_surcharge,\n    null as airport_fee\nfrom {{ source('bronze','green_trips') }}\n", "file_path": "/home/src/scheduler/dbt/models/staging/stg_green_trips.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/staging/stg_green_trips"}, "/home/src/scheduler/dbt/models/silver/taxi_zones.sql:dbt:sql:home/src/scheduler/dbt/models/silver/taxi zones": {"content": "{{ config(materialized='table') }}\n\nselect\n    locationid as location_id,\n    borough,\n    zone,\n    service_zone\nfrom {{ source('bronze','taxi_zones') }}\n", "file_path": "/home/src/scheduler/dbt/models/silver/taxi_zones.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/silver/taxi_zones"}, "/home/src/scheduler/dbt/models/silver/trips_all.sql:dbt:sql:home/src/scheduler/dbt/models/silver/trips all": {"content": "{{ config(materialized='table') }}\n\nwith yellow as (\n    select\n        vendorid           as vendor_id,\n        tpep_pickup_datetime  as pickup_datetime,\n        tpep_dropoff_datetime as dropoff_datetime,\n        passenger_count,\n        trip_distance,\n        ratecodeid         as rate_code_id,\n        store_and_fwd_flag,\n        pulocationid       as pickup_location_id,\n        dolocationid       as dropoff_location_id,\n        payment_type,\n        fare_amount,\n        extra,\n        mta_tax,\n        tip_amount,\n        tolls_amount,\n        improvement_surcharge,\n        total_amount,\n        congestion_surcharge,\n        airport_fee\n    from {{ source('bronze','yellow_trips') }}\n),\ngreen as (\n    select\n        vendorid           as vendor_id,\n        lpep_pickup_datetime  as pickup_datetime,\n        lpep_dropoff_datetime as dropoff_datetime,\n        passenger_count,\n        trip_distance,\n        ratecodeid         as rate_code_id,\n        store_and_fwd_flag,\n        pulocationid       as pickup_location_id,\n        dolocationid       as dropoff_location_id,\n        payment_type,\n        fare_amount,\n        extra,\n        mta_tax,\n        tip_amount,\n        tolls_amount,\n        improvement_surcharge,\n        total_amount,\n        congestion_surcharge,\n        null as airport_fee\n    from {{ source('bronze','green_trips') }}\n)\n\nselect * from yellow\nunion all\nselect * from green\n", "file_path": "/home/src/scheduler/dbt/models/silver/trips_all.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/silver/trips_all"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}